# 余弦相似度与 BM25 学习笔记（结合本项目实现）

本文在算法原理基础上，结合本项目的实现路径，说明余弦相似度与 BM25 的使用方式与适用场景，便于快速理解与对照源码。

## 1. 余弦相似度（Cosine Similarity）

### 1.1 定义

余弦相似度衡量两个向量在方向上的相似程度，取值范围为 [-1, 1]（文本向量通常为 [0, 1]）。核心思想是“只看方向，不看长度”。

### 1.2 数学公式

```text
cos(θ) = (A · B) / (||A|| * ||B||)
```

- A · B：向量点积
- ||A||：向量模长
- θ：向量夹角

### 1.3 直观理解

- 夹角越小 → 越相似
- 完全一致 → 相似度 = 1
- 完全无关 → 相似度 ≈ 0

### 1.4 文本检索中的应用

文本向量化常见方式：TF-IDF、Word2Vec、Sentence/Document Embedding。流程如下：

```text
Query → 向量化 → 余弦相似度 → 排序
```

### 1.5 优缺点

优点：

- 对文本长度不敏感
- 实现简单、计算高效
- 与向量检索生态兼容

缺点：

- 依赖向量质量
- 不理解词频结构
- 不适合纯关键词匹配

### 1.6 典型场景

- 语义搜索
- 相似问答匹配
- RAG 向量召回阶段
- 推荐系统

---

## 2. BM25（Best Matching 25）

### 2.1 定义

BM25 是基于概率模型的关键词检索排序算法，是 TF-IDF 的改进版本。核心思想：关键词匹配 + 词频控制 + 文档长度归一化。

### 2.2 核心公式

```text
BM25(q, d) = Σ IDF(t) * ( f(t, d) * (k1 + 1) )
            / ( f(t, d) + k1 * (1 - b + b * |d|/avgdl) )
```

- f(t, d)：词 t 在文档 d 中出现次数
- |d|：文档长度
- avgdl：平均文档长度
- k1：词频饱和参数（常用 1.2～2.0）
- b：长度惩罚参数（常用 0.75）
- IDF(t)：逆文档频率

### 2.3 核心机制拆解

- 词频饱和：词出现次数不会线性增加贡献
- 文档长度归一化：长文档不会天然占优
- 逆文档频率：稀有词权重更高

### 2.4 直观理解

BM25 在“词出现次数”与“刷词惩罚”之间做平衡。

### 2.5 优缺点

优点：

- 关键词检索效果稳定
- 不依赖向量模型
- 可解释性强

缺点：

- 不理解语义
- 对同义词、近义句无能为力
- 对拼写变化不友好

### 2.6 典型场景

- 传统全文检索
- 法律/文档/代码检索
- 关键词精确匹配系统
- 搜索引擎第一阶段召回

---

## 3. 余弦相似度 vs BM25


| 维度         | 余弦相似度      | BM25       |
| ------------ | --------------- | ---------- |
| 类型         | 向量相似度      | 关键词排序 |
| 是否理解语义 | 是（依赖向量）  | 否         |
| 是否依赖分词 | 否（Embedding） | 是         |
| 长度归一化   | 天然支持        | 显式支持   |
| 可解释性     | 低              | 高         |
| 工程复杂度   | 中              | 低         |

---

## 4. 工程实践中的组合

典型架构：

```text
Query
 ├─ BM25 → 关键词粗召回
 └─ 向量 + Cosine → 语义精排
```

或混合评分：

```text
Score = α * BM25 + β * CosineSimilarity
```

---

## 5. 本项目中两种算法的使用方式

### 5.1 余弦相似度在项目中的实现与调用

**核心实现**：`easeim-mcp-server/src/intelligence/SimilarityMatcher.ts`

- **向量构造**：
  - `calculateCosineSimilarity()` 使用词频向量（英文单词 + 中文单字/二元组）
  - `trainIDF()` + `getTFIDFVector()` 支持 TF-IDF 加权向量
- **相似度计算**：
  - `cosineSimilarity()` / `cosineSimilarityInstance()` 进行点积与模长归一化
- **文本预处理**：
  - 内置停用词过滤
  - 中英文混合分词，中文支持单字与二元组

**实际调用位置**：

- `easeim-mcp-server/src/intelligence/IntentClassifier.ts`
  - `matchSemanticScenario()` 使用 `SimilarityMatcher.findBestMatch()` 做意图兜底
  - 触发条件：当规则与实体识别置信度不足时，用余弦相似度进行语义补偿
  - 阈值：0.20（低阈值，倾向兜底召回，同时防止误匹配概率）

**效果定位**：

- 用于“语义匹配兜底”，不是主召回方式
- 不依赖向量模型，属于轻量级语义相似度估计

---

### 5.2 BM25 在项目中的实现与调用

**核心实现**：`easeim-mcp-server/src/search/InvertedIndex.ts`

- **索引构建**：
  - `build()`/`addDocument()`：字段级索引，记录词频、文档长度
  - 字段权重 `fieldWeights` 用于提升 name/title/keywords 的排序权重
- **IDF 预计算**：
  - `precomputeIDF()` 使用 BM25 变体 IDF：
    ```text
    idf = log((N - df + 0.5) / (df + 0.5) + 1)
    ```
- **BM25 搜索**：
  - `search()`：使用 k1=1.2、b=0.75
  - 评分公式中叠加字段权重

**实际调用位置**：

- `easeim-mcp-server/src/search/DocSearch.ts`
  - `searchApi()`：
    - 拼写纠错 → 查询扩展 → BM25 检索 → 平台过滤
  - `searchGuide()`：
    - 拼写纠错 → 查询扩展 → BM25 检索

**效果定位**：

- 用于“关键词检索主通道”，承担主召回排序
- 结合拼写纠错和同义词扩展，提高召回

---

## 6. 结论与建议

- 余弦相似度：本项目用于意图识别的语义兜底，解决规则覆盖不足的问题。
- BM25：本项目用于文档检索主流程，结合倒排索引与字段权重，实现可解释的关键词检索。
- 若后续引入向量检索，可考虑“BM25 召回 + Cosine 精排”的组合架构。
